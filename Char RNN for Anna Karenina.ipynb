{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network is based off of Andrej Karpathy's post on RNNs and implementation in Torch.The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina.  Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "##text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There are only 83 unique charecters here in my text including caps and lower case\n",
    "## So I am not using any dimension reduction here \n",
    "len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tuple(set(text))\n",
    "words_count = Counter(words)\n",
    "##sorted_words = sorted(words_count, key= words_count.get, reverse = True)\n",
    "\n",
    "## sorting not required here since this is a char RNN\n",
    "\n",
    "int2char = dict(enumerate(words_count))\n",
    "char2int = {ch:ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode the text\n",
    "\n",
    "encoded = []\n",
    "for ch in text:\n",
    "    encoded.append(char2int[ch])\n",
    "encoded = np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting each charecter into one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr,n_labels):\n",
    "    ##rows = np.multiply(arr.shape[0],arr.shape[1])\n",
    "    rows = np.multiply(*arr.shape)\n",
    "    one_hot = np.zeros((rows , n_labels ),dtype = np.int32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot= one_hot.reshape(arr.shape[0],arr.shape[1],n_labels)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = np.array([[1,2,3]])\n",
    "one_hot(test_seq, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We just want batches to fill all the batches. So the total length should be N * M * K\n",
    "###### where N = Batch_size , M = Seq_len K = No of Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_len):\n",
    "    total_batches = len(arr)//(batch_size*seq_len)\n",
    "    total_arr_length = batch_size*seq_len*total_batches  ## N * M * K\n",
    "    \n",
    "    arr =  arr [ : total_arr_length]\n",
    "    \n",
    "    ## We will use Batch First =True going forward\n",
    "    ## This would mean the no of columns is equal to M*K\n",
    "    ## So in one loop we will just yield M columns...and this would happen K times\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "    \n",
    "    for n in (0, arr.shape[1], seq_len):\n",
    "        x = arr[:,n:n+seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        print(y.shape)\n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = arr[:,n+seq_len]\n",
    "        except IndexError: ## for the last Seq_length\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = arr[:,0]\n",
    "            \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[11 50 43 19 33 44  5 51 27 28]\n",
      " [52 51 33 50 44 51 33  5 43 82]\n",
      " [66 82 58  8 51 50 82 68 51 23]\n",
      " [76 58 44 23 51 43 58 17 51 33]\n",
      " [44 26 51 60 37 33 51 50 44 51]\n",
      " [51 18 66 44 59 43 58 17  5 76]\n",
      " [51  3 76 33 50 51  3 44 31 43]\n",
      " [52 43  8 44 51 82 58 51 53 50]]\n",
      "\n",
      "y\n",
      " [[50 43 19 33 44  5 51 27 28 28]\n",
      " [51 33 50 44 51 33  5 43 82 58]\n",
      " [82 58  8 51 50 82 68 51 23 76]\n",
      " [58 44 23 51 43 58 17 51 33 76]\n",
      " [26 51 60 37 33 51 50 44 51 50]\n",
      " [18 66 44 59 43 58 17  5 76 69]\n",
      " [ 3 76 33 50 51  3 44 31 43 37]\n",
      " [43  8 44 51 82 58 51 53 50 82]]\n"
     ]
    }
   ],
   "source": [
    "##test get_batches \n",
    "\n",
    "batches = get_batches(encoded, 8,100)\n",
    "x,y = next(batches)\n",
    "\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "if gpu:\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Not Training on GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining the network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn. Module):\n",
    "    def __init__(self,tokens, n_hidden=256, n_layers=2,drop_prob = 0.5,lr = 0.001):\n",
    "        super().__init__()\n",
    "        ## Class variables\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        ## create class dictionaries \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n",
    "        \n",
    "        ## Create the layers \n",
    "        ## I am not using any embedding here \n",
    "        input_size = len(self.chars)\n",
    "        output_size = len(self.chars)\n",
    "        self.lstm = nn.LSTM(input_size, self.n_hidden, self.n_layers, \n",
    "                           dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.FC = nn.Linear(self.n_hidden, output_size)\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        r_out,hidden = self.lstm(x,hidden)\n",
    "        r_out = self.dropout(r_out)\n",
    "        \n",
    "        r_out = r_out.contiguous().reshape(-1,self.n_hidden)\n",
    "        \n",
    "        out = self.FC(r_out)\n",
    "        \n",
    "        return out,hidden\n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        ### initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weights = next(self.parameters()).data\n",
    "        \n",
    "        if gpu: ## Create Cuda tensors\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weights.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data,epochs =1,batch_size = 10,seq_len =10,lr = 0.001,clip = 5, val_frac= 0.1, print_freq = 10):\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    ##optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model_name = 'char_rnn.net'\n",
    "    \n",
    "    \n",
    "    ##Create train and Validation set \n",
    "    val_idx = int(len(data)*(1- val_frac))\n",
    "    train_data, valid_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    #### counter and vocab size\n",
    "    vocab_size = len(model.chars)\n",
    "    minimum_val_loss =np.inf\n",
    "    ##minimum_val_loss =3.1185\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        counter = 0\n",
    "        train\n",
    "        \n",
    "        h = model.init_hidden(batch_size)\n",
    "        for x,y in get_batches(train_data,batch_size,seq_len):\n",
    "            model.train()\n",
    "            counter +=1\n",
    "            x = one_hot(x,vocab_size)\n",
    "            inputs = torch.from_numpy(x)\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            labels = torch.from_numpy(y)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "            if gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            h = tuple([each.data for each in h]) ## this is already in cuda if GPU is available\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output,h = model(inputs,h)\n",
    "            labels = labels.type(torch.cuda.LongTensor)\n",
    "            loss = criterion(output,labels.view(batch_size*seq_len))\n",
    "            loss.backward()            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if counter% print_freq == 0:\n",
    "                model.eval()\n",
    "                val_losses =[]\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                for val_x,val_y in get_batches(valid_data,batch_size,seq_len):\n",
    "                    val_x= one_hot(val_x,vocab_size)\n",
    "                    inputs, labels = torch.from_numpy(val_x), torch.from_numpy(val_y)\n",
    "                    inputs = inputs.type(torch.FloatTensor)\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    inputs,labels = inputs.cuda(),labels.cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    output,val_h = model(inputs,val_h)\n",
    "                    labels = labels.type(torch.cuda.LongTensor)\n",
    "                    loss= criterion(output,labels.view(batch_size*seq_len))\n",
    "                    val_losses.append(loss.item())\n",
    "  \n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "    \n",
    "                ## Save Model if validation loss goes down\n",
    "                if (np.mean(val_losses) < minimum_val_loss):\n",
    "                    minimum_val_loss = np.mean(val_losses)\n",
    "                    checkpoint = {'n_hidden' : model.n_hidden,\n",
    "                                  'n_layers' : model.n_layers,\n",
    "                                   'state_dict': model.state_dict(),\n",
    "                                   'tokens' :model.chars}\n",
    "                    print('Saving Model...')\n",
    "        \n",
    "                with open(model_name, 'wb') as f:\n",
    "                    torch.save(checkpoint,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (FC): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##Hyperparameters\n",
    "\n",
    "n_hidden= 512\n",
    "n_layers=2\n",
    "drop_prob = 0.5\n",
    "lr = 0.001\n",
    "\n",
    "model = CharRNN(words,n_hidden,n_layers,drop_prob,lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 1.5611... Val Loss: 1.5153\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 20... Loss: 1.5503... Val Loss: 1.4999\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 30... Loss: 1.5450... Val Loss: 1.4926\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 40... Loss: 1.5386... Val Loss: 1.4889\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 50... Loss: 1.5375... Val Loss: 1.4872\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 60... Loss: 1.5360... Val Loss: 1.4828\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 70... Loss: 1.5350... Val Loss: 1.4835\n",
      "Epoch: 1/20... Step: 80... Loss: 1.5373... Val Loss: 1.4822\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 90... Loss: 1.5262... Val Loss: 1.4763\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 100... Loss: 1.5266... Val Loss: 1.4743\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 110... Loss: 1.5292... Val Loss: 1.4740\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 120... Loss: 1.5269... Val Loss: 1.4703\n",
      "Saving Model...\n",
      "Epoch: 1/20... Step: 130... Loss: 1.5204... Val Loss: 1.4689\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 10... Loss: 1.5140... Val Loss: 1.4634\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 20... Loss: 1.5130... Val Loss: 1.4605\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 30... Loss: 1.5154... Val Loss: 1.4604\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 40... Loss: 1.5121... Val Loss: 1.4583\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 50... Loss: 1.5107... Val Loss: 1.4576\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 60... Loss: 1.5090... Val Loss: 1.4543\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 70... Loss: 1.5072... Val Loss: 1.4553\n",
      "Epoch: 2/20... Step: 80... Loss: 1.5150... Val Loss: 1.4557\n",
      "Epoch: 2/20... Step: 90... Loss: 1.5003... Val Loss: 1.4478\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 100... Loss: 1.5024... Val Loss: 1.4495\n",
      "Epoch: 2/20... Step: 110... Loss: 1.4998... Val Loss: 1.4463\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 120... Loss: 1.5000... Val Loss: 1.4434\n",
      "Saving Model...\n",
      "Epoch: 2/20... Step: 130... Loss: 1.4950... Val Loss: 1.4418\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 10... Loss: 1.4906... Val Loss: 1.4364\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 20... Loss: 1.4952... Val Loss: 1.4382\n",
      "Epoch: 3/20... Step: 30... Loss: 1.4814... Val Loss: 1.4291\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 40... Loss: 1.4983... Val Loss: 1.4385\n",
      "Epoch: 3/20... Step: 50... Loss: 1.4871... Val Loss: 1.4321\n",
      "Epoch: 3/20... Step: 60... Loss: 1.4872... Val Loss: 1.4311\n",
      "Epoch: 3/20... Step: 70... Loss: 1.4886... Val Loss: 1.4320\n",
      "Epoch: 3/20... Step: 80... Loss: 1.4913... Val Loss: 1.4307\n",
      "Epoch: 3/20... Step: 90... Loss: 1.4739... Val Loss: 1.4231\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 100... Loss: 1.4757... Val Loss: 1.4221\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 110... Loss: 1.4766... Val Loss: 1.4207\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 120... Loss: 1.4762... Val Loss: 1.4185\n",
      "Saving Model...\n",
      "Epoch: 3/20... Step: 130... Loss: 1.4699... Val Loss: 1.4144\n",
      "Saving Model...\n",
      "Epoch: 4/20... Step: 10... Loss: 1.4763... Val Loss: 1.4158\n",
      "Epoch: 4/20... Step: 20... Loss: 1.4768... Val Loss: 1.4151\n",
      "Epoch: 4/20... Step: 30... Loss: 1.4656... Val Loss: 1.4089\n",
      "Saving Model...\n",
      "Epoch: 4/20... Step: 40... Loss: 1.4637... Val Loss: 1.4075\n",
      "Saving Model...\n",
      "Epoch: 4/20... Step: 50... Loss: 1.4691... Val Loss: 1.4115\n",
      "Epoch: 4/20... Step: 60... Loss: 1.4744... Val Loss: 1.4141\n",
      "Epoch: 4/20... Step: 70... Loss: 1.4655... Val Loss: 1.4089\n",
      "Epoch: 4/20... Step: 80... Loss: 1.4710... Val Loss: 1.4080\n",
      "Epoch: 4/20... Step: 90... Loss: 1.4578... Val Loss: 1.4017\n",
      "Saving Model...\n",
      "Epoch: 4/20... Step: 100... Loss: 1.4543... Val Loss: 1.4006\n",
      "Saving Model...\n",
      "Epoch: 4/20... Step: 110... Loss: 1.4539... Val Loss: 1.3998\n",
      "Saving Model...\n",
      "Epoch: 4/20... Step: 120... Loss: 1.4640... Val Loss: 1.4021\n",
      "Epoch: 4/20... Step: 130... Loss: 1.4513... Val Loss: 1.3948\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 10... Loss: 1.4478... Val Loss: 1.3940\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 20... Loss: 1.4559... Val Loss: 1.3923\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 30... Loss: 1.4511... Val Loss: 1.3894\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 40... Loss: 1.4583... Val Loss: 1.3948\n",
      "Epoch: 5/20... Step: 50... Loss: 1.4488... Val Loss: 1.3876\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 60... Loss: 1.4499... Val Loss: 1.3893\n",
      "Epoch: 5/20... Step: 70... Loss: 1.4465... Val Loss: 1.3874\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 80... Loss: 1.4513... Val Loss: 1.3878\n",
      "Epoch: 5/20... Step: 90... Loss: 1.4391... Val Loss: 1.3799\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 100... Loss: 1.4379... Val Loss: 1.3790\n",
      "Saving Model...\n",
      "Epoch: 5/20... Step: 110... Loss: 1.4492... Val Loss: 1.3855\n",
      "Epoch: 5/20... Step: 120... Loss: 1.4448... Val Loss: 1.3800\n",
      "Epoch: 5/20... Step: 130... Loss: 1.4372... Val Loss: 1.3757\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 10... Loss: 1.4369... Val Loss: 1.3741\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 20... Loss: 1.4348... Val Loss: 1.3700\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 30... Loss: 1.4332... Val Loss: 1.3676\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 40... Loss: 1.4349... Val Loss: 1.3696\n",
      "Epoch: 6/20... Step: 50... Loss: 1.4321... Val Loss: 1.3660\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 60... Loss: 1.4260... Val Loss: 1.3629\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 70... Loss: 1.4251... Val Loss: 1.3653\n",
      "Epoch: 6/20... Step: 80... Loss: 1.4352... Val Loss: 1.3682\n",
      "Epoch: 6/20... Step: 90... Loss: 1.4160... Val Loss: 1.3564\n",
      "Saving Model...\n",
      "Epoch: 6/20... Step: 100... Loss: 1.4201... Val Loss: 1.3566\n",
      "Epoch: 6/20... Step: 110... Loss: 1.4161... Val Loss: 1.3574\n",
      "Epoch: 6/20... Step: 120... Loss: 1.4229... Val Loss: 1.3590\n",
      "Epoch: 6/20... Step: 130... Loss: 1.4128... Val Loss: 1.3524\n",
      "Saving Model...\n",
      "Epoch: 7/20... Step: 10... Loss: 1.4138... Val Loss: 1.3538\n",
      "Epoch: 7/20... Step: 20... Loss: 1.4103... Val Loss: 1.3495\n",
      "Saving Model...\n",
      "Epoch: 7/20... Step: 30... Loss: 1.4090... Val Loss: 1.3452\n",
      "Saving Model...\n",
      "Epoch: 7/20... Step: 40... Loss: 1.4156... Val Loss: 1.3543\n",
      "Epoch: 7/20... Step: 50... Loss: 1.4128... Val Loss: 1.3460\n",
      "Epoch: 7/20... Step: 60... Loss: 1.4119... Val Loss: 1.3491\n",
      "Epoch: 7/20... Step: 70... Loss: 1.4003... Val Loss: 1.3412\n",
      "Saving Model...\n",
      "Epoch: 7/20... Step: 80... Loss: 1.4077... Val Loss: 1.3439\n",
      "Epoch: 7/20... Step: 90... Loss: 1.3982... Val Loss: 1.3376\n",
      "Saving Model...\n",
      "Epoch: 7/20... Step: 100... Loss: 1.4014... Val Loss: 1.3395\n",
      "Epoch: 7/20... Step: 110... Loss: 1.4020... Val Loss: 1.3413\n",
      "Epoch: 7/20... Step: 120... Loss: 1.4056... Val Loss: 1.3388\n",
      "Epoch: 7/20... Step: 130... Loss: 1.4003... Val Loss: 1.3374\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 10... Loss: 1.4037... Val Loss: 1.3433\n",
      "Epoch: 8/20... Step: 20... Loss: 1.3957... Val Loss: 1.3338\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 30... Loss: 1.3995... Val Loss: 1.3360\n",
      "Epoch: 8/20... Step: 40... Loss: 1.3904... Val Loss: 1.3310\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 50... Loss: 1.3975... Val Loss: 1.3372\n",
      "Epoch: 8/20... Step: 60... Loss: 1.3985... Val Loss: 1.3373\n",
      "Epoch: 8/20... Step: 70... Loss: 1.3910... Val Loss: 1.3314\n",
      "Epoch: 8/20... Step: 80... Loss: 1.3943... Val Loss: 1.3289\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 90... Loss: 1.3877... Val Loss: 1.3258\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 100... Loss: 1.3855... Val Loss: 1.3240\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 110... Loss: 1.3870... Val Loss: 1.3265\n",
      "Epoch: 8/20... Step: 120... Loss: 1.3860... Val Loss: 1.3235\n",
      "Saving Model...\n",
      "Epoch: 8/20... Step: 130... Loss: 1.3805... Val Loss: 1.3204\n",
      "Saving Model...\n",
      "Epoch: 9/20... Step: 10... Loss: 1.3854... Val Loss: 1.3241\n",
      "Epoch: 9/20... Step: 20... Loss: 1.3839... Val Loss: 1.3225\n",
      "Epoch: 9/20... Step: 30... Loss: 1.3803... Val Loss: 1.3176\n",
      "Saving Model...\n",
      "Epoch: 9/20... Step: 40... Loss: 1.3806... Val Loss: 1.3197\n",
      "Epoch: 9/20... Step: 50... Loss: 1.3815... Val Loss: 1.3226\n",
      "Epoch: 9/20... Step: 60... Loss: 1.3823... Val Loss: 1.3222\n",
      "Epoch: 9/20... Step: 70... Loss: 1.3812... Val Loss: 1.3214\n",
      "Epoch: 9/20... Step: 80... Loss: 1.3855... Val Loss: 1.3225\n",
      "Epoch: 9/20... Step: 90... Loss: 1.3769... Val Loss: 1.3146\n",
      "Saving Model...\n",
      "Epoch: 9/20... Step: 100... Loss: 1.3763... Val Loss: 1.3135\n",
      "Saving Model...\n",
      "Epoch: 9/20... Step: 110... Loss: 1.3837... Val Loss: 1.3186\n",
      "Epoch: 9/20... Step: 120... Loss: 1.3842... Val Loss: 1.3176\n",
      "Epoch: 9/20... Step: 130... Loss: 1.3773... Val Loss: 1.3123\n",
      "Saving Model...\n",
      "Epoch: 10/20... Step: 10... Loss: 1.3734... Val Loss: 1.3119\n",
      "Saving Model...\n",
      "Epoch: 10/20... Step: 20... Loss: 1.3735... Val Loss: 1.3102\n",
      "Saving Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 30... Loss: 1.3652... Val Loss: 1.3046\n",
      "Saving Model...\n",
      "Epoch: 10/20... Step: 40... Loss: 1.3716... Val Loss: 1.3087\n",
      "Epoch: 10/20... Step: 50... Loss: 1.3786... Val Loss: 1.3111\n",
      "Epoch: 10/20... Step: 60... Loss: 1.3806... Val Loss: 1.3138\n",
      "Epoch: 10/20... Step: 70... Loss: 1.3746... Val Loss: 1.3108\n",
      "Epoch: 10/20... Step: 80... Loss: 1.3796... Val Loss: 1.3130\n",
      "Epoch: 10/20... Step: 90... Loss: 1.3746... Val Loss: 1.3073\n",
      "Epoch: 10/20... Step: 100... Loss: 1.3723... Val Loss: 1.3051\n",
      "Epoch: 10/20... Step: 110... Loss: 1.3691... Val Loss: 1.3034\n",
      "Saving Model...\n",
      "Epoch: 10/20... Step: 120... Loss: 1.3722... Val Loss: 1.3027\n",
      "Saving Model...\n",
      "Epoch: 10/20... Step: 130... Loss: 1.3677... Val Loss: 1.3004\n",
      "Saving Model...\n",
      "Epoch: 11/20... Step: 10... Loss: 1.3715... Val Loss: 1.3029\n",
      "Epoch: 11/20... Step: 20... Loss: 1.3721... Val Loss: 1.3024\n",
      "Epoch: 11/20... Step: 30... Loss: 1.3683... Val Loss: 1.3011\n",
      "Epoch: 11/20... Step: 40... Loss: 1.3757... Val Loss: 1.3079\n",
      "Epoch: 11/20... Step: 50... Loss: 1.3702... Val Loss: 1.3016\n",
      "Epoch: 11/20... Step: 60... Loss: 1.3709... Val Loss: 1.3018\n",
      "Epoch: 11/20... Step: 70... Loss: 1.3718... Val Loss: 1.3049\n",
      "Epoch: 11/20... Step: 80... Loss: 1.3719... Val Loss: 1.3018\n",
      "Epoch: 11/20... Step: 90... Loss: 1.3647... Val Loss: 1.2963\n",
      "Saving Model...\n",
      "Epoch: 11/20... Step: 100... Loss: 1.3682... Val Loss: 1.2996\n",
      "Epoch: 11/20... Step: 110... Loss: 1.3648... Val Loss: 1.2972\n",
      "Epoch: 11/20... Step: 120... Loss: 1.3739... Val Loss: 1.3033\n",
      "Epoch: 11/20... Step: 130... Loss: 1.3679... Val Loss: 1.2976\n",
      "Epoch: 12/20... Step: 10... Loss: 1.3586... Val Loss: 1.2947\n",
      "Saving Model...\n",
      "Epoch: 12/20... Step: 20... Loss: 1.3612... Val Loss: 1.2941\n",
      "Saving Model...\n",
      "Epoch: 12/20... Step: 30... Loss: 1.3574... Val Loss: 1.2908\n",
      "Saving Model...\n",
      "Epoch: 12/20... Step: 40... Loss: 1.3621... Val Loss: 1.2946\n",
      "Epoch: 12/20... Step: 50... Loss: 1.3621... Val Loss: 1.2949\n",
      "Epoch: 12/20... Step: 60... Loss: 1.3589... Val Loss: 1.2904\n",
      "Saving Model...\n",
      "Epoch: 12/20... Step: 70... Loss: 1.3604... Val Loss: 1.2956\n",
      "Epoch: 12/20... Step: 80... Loss: 1.3679... Val Loss: 1.2968\n",
      "Epoch: 12/20... Step: 90... Loss: 1.3540... Val Loss: 1.2889\n",
      "Saving Model...\n",
      "Epoch: 12/20... Step: 100... Loss: 1.3611... Val Loss: 1.2942\n",
      "Epoch: 12/20... Step: 110... Loss: 1.3636... Val Loss: 1.2943\n",
      "Epoch: 12/20... Step: 120... Loss: 1.3631... Val Loss: 1.2903\n",
      "Epoch: 12/20... Step: 130... Loss: 1.3620... Val Loss: 1.2906\n",
      "Epoch: 13/20... Step: 10... Loss: 1.3571... Val Loss: 1.2898\n",
      "Epoch: 13/20... Step: 20... Loss: 1.3519... Val Loss: 1.2868\n",
      "Saving Model...\n",
      "Epoch: 13/20... Step: 30... Loss: 1.3495... Val Loss: 1.2846\n",
      "Saving Model...\n",
      "Epoch: 13/20... Step: 40... Loss: 1.3523... Val Loss: 1.2857\n",
      "Epoch: 13/20... Step: 50... Loss: 1.3573... Val Loss: 1.2886\n",
      "Epoch: 13/20... Step: 60... Loss: 1.3535... Val Loss: 1.2858\n",
      "Epoch: 13/20... Step: 70... Loss: 1.3491... Val Loss: 1.2843\n",
      "Saving Model...\n",
      "Epoch: 13/20... Step: 80... Loss: 1.3635... Val Loss: 1.2915\n",
      "Epoch: 13/20... Step: 90... Loss: 1.3598... Val Loss: 1.2888\n",
      "Epoch: 13/20... Step: 100... Loss: 1.3667... Val Loss: 1.2945\n",
      "Epoch: 13/20... Step: 110... Loss: 1.3611... Val Loss: 1.2902\n",
      "Epoch: 13/20... Step: 120... Loss: 1.3563... Val Loss: 1.2841\n",
      "Saving Model...\n",
      "Epoch: 13/20... Step: 130... Loss: 1.3586... Val Loss: 1.2861\n",
      "Epoch: 14/20... Step: 10... Loss: 1.3461... Val Loss: 1.2798\n",
      "Saving Model...\n",
      "Epoch: 14/20... Step: 20... Loss: 1.3502... Val Loss: 1.2853\n",
      "Epoch: 14/20... Step: 30... Loss: 1.3496... Val Loss: 1.2806\n",
      "Epoch: 14/20... Step: 40... Loss: 1.3500... Val Loss: 1.2827\n",
      "Epoch: 14/20... Step: 50... Loss: 1.3519... Val Loss: 1.2838\n",
      "Epoch: 14/20... Step: 60... Loss: 1.3502... Val Loss: 1.2815\n",
      "Epoch: 14/20... Step: 70... Loss: 1.3495... Val Loss: 1.2819\n",
      "Epoch: 14/20... Step: 80... Loss: 1.3547... Val Loss: 1.2832\n",
      "Epoch: 14/20... Step: 90... Loss: 1.3441... Val Loss: 1.2793\n",
      "Saving Model...\n",
      "Epoch: 14/20... Step: 100... Loss: 1.3524... Val Loss: 1.2829\n",
      "Epoch: 14/20... Step: 110... Loss: 1.3481... Val Loss: 1.2815\n",
      "Epoch: 14/20... Step: 120... Loss: 1.3502... Val Loss: 1.2804\n",
      "Epoch: 14/20... Step: 130... Loss: 1.3551... Val Loss: 1.2797\n",
      "Epoch: 15/20... Step: 10... Loss: 1.3523... Val Loss: 1.2811\n",
      "Epoch: 15/20... Step: 20... Loss: 1.3380... Val Loss: 1.2756\n",
      "Saving Model...\n",
      "Epoch: 15/20... Step: 30... Loss: 1.3479... Val Loss: 1.2773\n",
      "Epoch: 15/20... Step: 40... Loss: 1.3432... Val Loss: 1.2753\n",
      "Saving Model...\n",
      "Epoch: 15/20... Step: 50... Loss: 1.3477... Val Loss: 1.2773\n",
      "Epoch: 15/20... Step: 60... Loss: 1.3535... Val Loss: 1.2771\n",
      "Epoch: 15/20... Step: 70... Loss: 1.3404... Val Loss: 1.2721\n",
      "Saving Model...\n",
      "Epoch: 15/20... Step: 80... Loss: 1.3473... Val Loss: 1.2755\n",
      "Epoch: 15/20... Step: 90... Loss: 1.3382... Val Loss: 1.2704\n",
      "Saving Model...\n",
      "Epoch: 15/20... Step: 100... Loss: 1.3492... Val Loss: 1.2789\n",
      "Epoch: 15/20... Step: 110... Loss: 1.3561... Val Loss: 1.2828\n",
      "Epoch: 15/20... Step: 120... Loss: 1.3492... Val Loss: 1.2748\n",
      "Epoch: 15/20... Step: 130... Loss: 1.3471... Val Loss: 1.2742\n",
      "Epoch: 16/20... Step: 10... Loss: 1.3470... Val Loss: 1.2741\n",
      "Epoch: 16/20... Step: 20... Loss: 1.3447... Val Loss: 1.2721\n",
      "Epoch: 16/20... Step: 30... Loss: 1.3415... Val Loss: 1.2695\n",
      "Saving Model...\n",
      "Epoch: 16/20... Step: 40... Loss: 1.3400... Val Loss: 1.2704\n",
      "Epoch: 16/20... Step: 50... Loss: 1.3442... Val Loss: 1.2739\n",
      "Epoch: 16/20... Step: 60... Loss: 1.3396... Val Loss: 1.2685\n",
      "Saving Model...\n",
      "Epoch: 16/20... Step: 70... Loss: 1.3391... Val Loss: 1.2712\n",
      "Epoch: 16/20... Step: 80... Loss: 1.3446... Val Loss: 1.2732\n",
      "Epoch: 16/20... Step: 90... Loss: 1.3382... Val Loss: 1.2669\n",
      "Saving Model...\n",
      "Epoch: 16/20... Step: 100... Loss: 1.3418... Val Loss: 1.2690\n",
      "Epoch: 16/20... Step: 110... Loss: 1.3495... Val Loss: 1.2775\n",
      "Epoch: 16/20... Step: 120... Loss: 1.3456... Val Loss: 1.2693\n",
      "Epoch: 16/20... Step: 130... Loss: 1.3426... Val Loss: 1.2690\n",
      "Epoch: 17/20... Step: 10... Loss: 1.3375... Val Loss: 1.2686\n",
      "Epoch: 17/20... Step: 20... Loss: 1.3387... Val Loss: 1.2690\n",
      "Epoch: 17/20... Step: 30... Loss: 1.3418... Val Loss: 1.2692\n",
      "Epoch: 17/20... Step: 40... Loss: 1.3438... Val Loss: 1.2691\n",
      "Epoch: 17/20... Step: 50... Loss: 1.3409... Val Loss: 1.2735\n",
      "Epoch: 17/20... Step: 60... Loss: 1.3385... Val Loss: 1.2673\n",
      "Epoch: 17/20... Step: 70... Loss: 1.3409... Val Loss: 1.2697\n",
      "Epoch: 17/20... Step: 80... Loss: 1.3433... Val Loss: 1.2688\n",
      "Epoch: 17/20... Step: 90... Loss: 1.3420... Val Loss: 1.2655\n",
      "Saving Model...\n",
      "Epoch: 17/20... Step: 100... Loss: 1.3403... Val Loss: 1.2653\n",
      "Saving Model...\n",
      "Epoch: 17/20... Step: 110... Loss: 1.3370... Val Loss: 1.2664\n",
      "Epoch: 17/20... Step: 120... Loss: 1.3372... Val Loss: 1.2664\n",
      "Epoch: 17/20... Step: 130... Loss: 1.3441... Val Loss: 1.2718\n",
      "Epoch: 18/20... Step: 10... Loss: 1.3422... Val Loss: 1.2683\n",
      "Epoch: 18/20... Step: 20... Loss: 1.3378... Val Loss: 1.2645\n",
      "Saving Model...\n",
      "Epoch: 18/20... Step: 30... Loss: 1.3338... Val Loss: 1.2614\n",
      "Saving Model...\n",
      "Epoch: 18/20... Step: 40... Loss: 1.3325... Val Loss: 1.2626\n",
      "Epoch: 18/20... Step: 50... Loss: 1.3318... Val Loss: 1.2637\n",
      "Epoch: 18/20... Step: 60... Loss: 1.3338... Val Loss: 1.2621\n",
      "Epoch: 18/20... Step: 70... Loss: 1.3312... Val Loss: 1.2625\n",
      "Epoch: 18/20... Step: 80... Loss: 1.3346... Val Loss: 1.2662\n",
      "Epoch: 18/20... Step: 90... Loss: 1.3330... Val Loss: 1.2612\n",
      "Saving Model...\n",
      "Epoch: 18/20... Step: 100... Loss: 1.3254... Val Loss: 1.2595\n",
      "Saving Model...\n",
      "Epoch: 18/20... Step: 110... Loss: 1.3401... Val Loss: 1.2674\n",
      "Epoch: 18/20... Step: 120... Loss: 1.3388... Val Loss: 1.2621\n",
      "Epoch: 18/20... Step: 130... Loss: 1.3439... Val Loss: 1.2670\n",
      "Epoch: 19/20... Step: 10... Loss: 1.3317... Val Loss: 1.2637\n",
      "Epoch: 19/20... Step: 20... Loss: 1.3328... Val Loss: 1.2634\n",
      "Epoch: 19/20... Step: 30... Loss: 1.3231... Val Loss: 1.2557\n",
      "Saving Model...\n",
      "Epoch: 19/20... Step: 40... Loss: 1.3306... Val Loss: 1.2613\n",
      "Epoch: 19/20... Step: 50... Loss: 1.3301... Val Loss: 1.2612\n",
      "Epoch: 19/20... Step: 60... Loss: 1.3348... Val Loss: 1.2603\n",
      "Epoch: 19/20... Step: 70... Loss: 1.3418... Val Loss: 1.2644\n",
      "Epoch: 19/20... Step: 80... Loss: 1.3441... Val Loss: 1.2677\n",
      "Epoch: 19/20... Step: 90... Loss: 1.3397... Val Loss: 1.2616\n",
      "Epoch: 19/20... Step: 100... Loss: 1.3329... Val Loss: 1.2572\n",
      "Epoch: 19/20... Step: 110... Loss: 1.3440... Val Loss: 1.2654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20... Step: 120... Loss: 1.3386... Val Loss: 1.2627\n",
      "Epoch: 19/20... Step: 130... Loss: 1.3310... Val Loss: 1.2568\n",
      "Epoch: 20/20... Step: 10... Loss: 1.3253... Val Loss: 1.2554\n",
      "Saving Model...\n",
      "Epoch: 20/20... Step: 20... Loss: 1.3376... Val Loss: 1.2598\n",
      "Epoch: 20/20... Step: 30... Loss: 1.3383... Val Loss: 1.2600\n",
      "Epoch: 20/20... Step: 40... Loss: 1.3346... Val Loss: 1.2572\n",
      "Epoch: 20/20... Step: 50... Loss: 1.3317... Val Loss: 1.2582\n",
      "Epoch: 20/20... Step: 60... Loss: 1.3340... Val Loss: 1.2559\n",
      "Epoch: 20/20... Step: 70... Loss: 1.3359... Val Loss: 1.2606\n",
      "Epoch: 20/20... Step: 80... Loss: 1.3403... Val Loss: 1.2621\n",
      "Epoch: 20/20... Step: 90... Loss: 1.3348... Val Loss: 1.2582\n",
      "Epoch: 20/20... Step: 100... Loss: 1.3368... Val Loss: 1.2600\n",
      "Epoch: 20/20... Step: 110... Loss: 1.3414... Val Loss: 1.2638\n",
      "Epoch: 20/20... Step: 120... Loss: 1.3377... Val Loss: 1.2595\n",
      "Epoch: 20/20... Step: 130... Loss: 1.3305... Val Loss: 1.2574\n"
     ]
    }
   ],
   "source": [
    "data= encoded\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "seq_len = 100\n",
    "lr = 0.001\n",
    "clip = 5\n",
    "val_frac= 0.1\n",
    "##print_freq = 10\n",
    "\n",
    "##Train the model\n",
    "train(model,data,epochs,batch_size,seq_len,lr,clip, val_frac= 0.1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "with open('char_rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "model = CharRNN(checkpoint['tokens'],n_hidden =checkpoint['n_hidden'],n_layers=checkpoint['n_layers'])\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction using this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char,h = None, top_k = None):\n",
    "    ## char is the input charecter here\n",
    "    ##model.eval() (while call eval during priming)\n",
    "    vocab_size = len(model.chars)\n",
    "    x = np.array([[char2int[char]]]) ## multipli dimentional\n",
    "    x = one_hot(x,vocab_size)\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x.type(torch.FloatTensor)\n",
    "    if gpu:\n",
    "        x= x.cuda()\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    output, h = model(x,h)\n",
    "    p= F.softmax(output,dim =1).data\n",
    "    if gpu:\n",
    "        p = p.cpu()\n",
    "    if top_k ==None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch= top_ch.numpy().squeeze()\n",
    "    p = p.numpy().squeeze()\n",
    "    ch = np.random.choice(top_ch, p = p/p.sum())\n",
    "    character = int2char[ch]\n",
    "    return character,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Priming and generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, prime='The', top_k=None):\n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1)  ## Batch size is 1 \n",
    "    model.eval()\n",
    "    for ch in prime:\n",
    "        char,h = predict(model, ch,h , top_k = top_k)\n",
    "    chars.append(char) ## just add the last one after prime\n",
    "    \n",
    "    for i in range(size):\n",
    "        char,h = predict(model, chars[-1],h , top_k = top_k)\n",
    "        chars.append(char)\n",
    "        \n",
    "    return ''.join(chars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text =generate_text(model, size=1000, prime='You are', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are to blame in\n",
      "the morning and happened, and what has said: \"You will be delighted at your\n",
      "acquainteninc.\"\n",
      "\n",
      "\"I see it in her eyes as I see anything.\"\n",
      "\n",
      "\"Yes, I should have been to be to care to see, to speak of him. But\n",
      "I see it's a care. Take their marshal to the thing of it.\"\n",
      "\n",
      "\"Yes,\" he said. \"I have not meant. You see it, are they have so partional of\n",
      "me, then I settled him about her in work in the carrying myself of\n",
      "all sours, and have that to make out when this is so much. I so good\n",
      "to be a look of still. And these all mind towards the story of his marriage,\n",
      "when I don't understand him. I've been an arm or the child, but what a discussion with\n",
      "the there there is in those though they did never come or work\n",
      "in his brother's feeling at the book as I should have so much,\" said\n",
      "Levin, with the secretary who came to her trough, and as a conditions of\n",
      "those chest--that was saying that he could not say in a sincerity, who show\n",
      "them, and without standing that he had terror and that she was not\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HAHA!! It was fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
